{% import 'macros.j2' as macros -%}

## Automation With k8s.mk
<hr style="width:100%;border-bottom:3px solid black;">

As an automation library or stand-alone tool, `k8s.mk` can help to create project-specific APIs that use the tool containers described in `k8s-tools.yml`.  Most of what's offered is in the form of **targets**, which you can directly use from the command line, or use as part of normal tasks/prereqs inside your project Makefile.  In other words, **the API and the CLI are the same**.  

Many targets are [scaffolded](#tool-container-basics), i.e. automatically generated for each tool container that's available.  Others targets are static, basically composing more primitive functionality to make it suitable for orchestration tasks.  Static targets usually fall into one of a few basic categories:

1. **Reusable implementations for common cluster automation tasks,** like [waiting for pods to get ready]({{mkdocs.site_relative_url}}/api#k8s.wait)
1. **Context-management tasks,** (like [setting the currently active namespace]({{mkdocs.site_relative_url}}/api#k8snamespacearg))
1. **Interactive debugging tasks,** (like [shelling into a new or existing pod inside some namespace]({{mkdocs.site_relative_url}}/api#k8sshellarg))

--------------------------------

This section is a quick overview of some of the capabilities, using a combination of working CLI-style examples, and *snippets* of scripts to illustrate ideas.

If you're looking instead for an API reference, [that is here]({{mkdocs.site_relative_url}}/api/#api-k8smk)).

If you want to focus on scripting and prefer to see **concrete end-to-end examples**, start instead with the demos, such as the [Cluster Lifecycle Demo]({{mkdocs.site_relative_url}}/demos#demo-cluster-automation).

### Tool Container Basics
<hr style="width:100%;border-bottom:3px solid black;">

Let's start with the basic stuff.  Commands in this section are all examples of *generic target scaffolding* that is generated automatically for all tool containers, including any containers that are user-defined later.  This functionality is inherited from [`compose.mk`]({{jinja.vars.composemk_docs_url}}).  Full documentation for scaffolding is out of scope here since it really isn't kubernetes specific, but it's worth covering briefly.  See the upstream docs for [more details]({{jinja.vars.composemk_docs_url}}/bridge/#target-scaffolding).

Other external documentation might also be of interest.  Combining `k8s.mk` helpers with the `compose.mk` [workflow support]({{jinja.vars.composemk_docs_url}}/standard-lib/#workflow-support) is often useful, since you can easily accomplish things like retries, delayed actions, conditional actions and lots of other stuff.  By using the [`tux.*` API]({{jinja.vars.composemk_docs_url}}/embedded-tui/) you can send tasks, or groups of tasks, into panes on a TUI.

#### Listing Available Tools 
<hr style="width:95%;border-bottom:1px dashed black;">

First, how about getting a list of tool containers that are defined?

```bash 
$ ./k8s.mk k8s-tools.services
{{bash("./k8s.mk k8s-tools.services").strip()}}
```

#### Shells & Task Dispatch
<hr style="width:95%;border-bottom:1px dashed black;">

Ok, what about shelling into a tool container to use commands directly?  We can do that and more, thanks to lots of generic targets that are generated for each container.

```bash
# Interactive shell into the kubectl container
$ ./k8s.mk kubectl.shell

# Send commands to the rancher container
$ echo rancher --version | ./k8s.mk rancher.shell.pipe
rancher version v2.8.4

# Dispatch any existing `make` target inside the given container
$ label="hello world" ./k8s.mk k3d.dispatch/io.print.banner
```

Dispatching tasks inside containers is a core feature of `compose.mk`, basically allowing for something like "agents" in jenkins, or "jobs" in Github Actions without locking your automation up inside proprietary platforms or file formats.  The k8s-tools suite organizes around this idea, describing tasks in `k8s.mk`, containers in `k8s-tools.yml`, and leveraging dispatch to accomplish "runs anywhere" style orchestration that you can then extend and compose inside your own scripts.

#### Container Meta
<hr style="width:95%;border-bottom:1px dashed black;">

Besides command execution and dispatch, there are various commands that operate on the containers themselves.  These roughly follow the `docker compose ..` verbs.  See the main docs [here]({{jinja.vars.composemk_docs_url}}/bridge/#top-level-container-handles)

```bash 
# Force a rebuild of the helm container, even if it's cached.
$ force=1 ./k8s.mk helm.build

# Show running kubefwd instances 
$ ./k8s.mk kubefwd.ps 
```
<br/>

### Cluster CRUD
<hr style="width:100%;border-bottom:3px solid black;">

For creating, updating, and deleting clusters, the  [k3d API]({{mkdocs.site_relative_url}}/api/#api-k3d) and the [kind API]({{mkdocs.site_relative_url}}/api/#api-k3d) are the main resources you're probably interested in.  

Using `k3d` is usually good enough and requires no external configuration for node-groups, so that is usually the best choice.  Jump to the [Cluster Lifecycle Demo]({{mkdocs.site_relative_url}}/demos#demo-cluster-automation) for a more substantial demo with scripting, but read on for the basics.

```bash 
# Prep a blank kubeconfig 
$ touch mycluster.conf; chmod 711 mycluster.conf; export KUBECONFIG=./mycluster.conf 

# Bootstrap a named cluster
$ ./k8s.mk k3d.cluster.get_or_create/mycluster

⇄ k3d.cluster.get_or_create/.. // Invoked from top; rerouting to tool-container 
⑆ k3d.cluster.get_or_create // mycluster 
Φ flux.if.then // flux.negate/k3d.has_cluster/mycluster .. true 
Φ flux.if.then // k3d.cluster.create/mycluster
⑆ k3d.cluster.create // mycluster 
+ k3d cluster create mycluster --servers 3 --agents 3 --api-port 6551 --volume /workspace/:/mycluster@all --wait
...

# Check it out
$ ./k8s.mk k3d.cluster.list k3d.stat

# Wipe it out.  (Skip this if you're using it in the next demos)
$ ./k8s.mk k3d.cluster.delete/mycluster
```

Interesting, right? A stand alone, project local cluster where even the nodes are dockerized, and without even installing `k3d` as a dependency.

### Basic Platforming
<hr style="width:100%;border-bottom:3px solid black;">

For this section, `KUBECONFIG` should already be set, and you might want to use the cluster from the last section.

#### Helm
<hr style="width:95%;border-bottom:1px dashed black;">

With the k8s-tools suite, you can use `helm` directly but the simplest and most idempotent approach to installing things is usually via {{macros.api_link('ansible.helm', mkdocs)}}.  *(See also the [upstream ansible docs](https://docs.ansible.com/ansible/latest/collections/kubernetes/core/helm_module.html))*

{#Again, if you're more interested in scripting you might prefer to skip to the [Cluster Lifecycle Demo]({{mkdocs.site_relative_url}}/demos#demo-cluster-automation), but here we'll use the CLI.#}

We'll also need JSON input for this part, which `k8s.mk` inherits via [compose.mk]({{jinja.vars.composemk_docs_url}}standard-lib/#structured-io), by way of the json.bash tool[^1].  As usual, no host dependencies are for this tool, or helm, or ansible, and everything is done via docker.  

The actual output is colored and easier to parse, but usage looks like this:

```bash
# Test out the jb tool.
$ ./k8s.mk jb name=ahoy \
    release_namespace=default \
    chart_ref=hello-world \
    chart_repo_url="https://helm.github.io/examples"

{ .. }

# Pipe JSON to helm
$ ./k8s.mk jb name=ahoy \
    release_namespace=default \
    chart_ref=hello-world \
    chart_repo_url="https://helm.github.io/examples" \
  | ./k8s.mk ansible.helm
  
⑆ ansible // kubernetes.core.helm // ← 
{
"name": "ahoy",
"release_namespace": "default",
"chart_ref": "hello-world",
"chart_repo_url": "https://helm.github.io/examples"
}
⑆ ansible // kubernetes.core.helm // →
{
  "changed": true,
  "action": "kubernetes.core.helm",
  "stats": {
    "changed": 1,
    "ok": 1
  }
}
```

Tidy. We took advantage of a discrete and useful piece of ansible without installing python, or ansible, or getting sucked into a never-ending process of ansibling all the things.

#### Blocking And Paralellism
<hr style="width:95%;border-bottom:1px dashed black;">

Since kubernetes is a distributed system, we don't necessarily want to be restricted to sequential activities in our scripts.  There are several ways to go about this, and lots of utilities in the k8s-tools suite will individually default to async and optionally support some kind of `--wait` argument. 

Actually the helm commands in the last section can be turned into blocking ones by simply passing `wait=true`, but suppose you don't know this though, or the tool doesn't support it, or orchestration is much more complex.  In this case and in many others, {{macros.api_link('k8s.wait', mkdocs)}} is your best friend.

Changing the previous helm example to wait for the whole cluster to settle looks like this:

```bash
$ ./k8s.mk jb name=ahoy \
    release_namespace=default \
    chart_ref=hello-world \
    chart_repo_url="https://helm.github.io/examples" \
  | ./k8s.mk ansible.helm \
  && ./k8s.mk k8s.wait
```

This shows a looping status display which shows exactly what's still pending and what the cluster is up to.  Under the hood, it uses a colorized, looping version of krew's kubectl-sick-pods plugin[^2].

In a similar fashion you can use {{macros.api_link('k8s.namespace.wait', mkdocs,arg='namespace')}} to block on one particular namespace, which opens up more interesting possibilities.  

As highlighted elsewhere, the nature of Makefile ensures that the `k8s.mk` CLI is essentially one-to-one with the API.  So thanks to that, **tools like `k8s.wait` aren't just commands, but also a powerful synchronization primitive in scripting**, especially when they are used as target prerequisites.  Consider the following snippet of Makefile:

```Makefile
# Create and enter the `my_platform` namespace before we do anything
setup.platform:
	kubectl apply ...

# Blocks on `my_platform` being paved, then sets namespace as default and continues
setup.app: k8s.namespace.wait/my_platform 
	kubectl apply ...

bootstrap: setup.platform setup.logging setup.monitoring setup.app
```

Of course you could do this in pure shell or a variety of other ways.  Preferring a Makefile tends to keep things really organized though, and we retain the ability to execute every piece of it individually, rather than being stuck with some kind of top-to-bottom execution of everything.  Much more advanced stuff is possible by combining make's [native support for parallelism](https://www.gnu.org/software/make/manual/html_node/Parallel.html) and its own [synchronization primitives](https://www.gnu.org/software/make/manual/html_node/Parallel-Disable.html).


<hr style="width:100%;border-bottom:3px solid black;">

[^1]: [https://github.com/h4l/json.bash](https://github.com/h4l/json.bash)
[^2]: [https://github.com/alecjacobs5401/kubectl-sick-pods](https://github.com/alecjacobs5401/kubectl-sick-pods)
{#
#### Istioctl
<hr style="width:95%;border-bottom:1px dashed black;">

```bash
$ echo istioctl install --set profile=demo -y | ./k8s.mk istioctl.shell.pipe
```


### Automation APIs over Tool Containers
<hr style="width:100%;border-bottom:3px solid black;">

What *is* an automation API over a tool container anyway?  

As an example, let's consider the [`kubectl.get` target]({{mkdocs.site_relative_url}}/api/#k8sget), which you might use like this:

```bash
# Usage: kubectl.get/<namespace>/<kind>/<name>/<filter>
$ KUBECONFIG=.. ./k8s.mk kubectl.get/argo-events/svc/webhook-eventsource-svc/.spec.clusterIP

# roughly equivalent to:
$ kubectl get $${kind} $${name} -n $${namespace} -o json | jq -r ..filter.."
```

The first command has no host requirements for `kubectl` or `jq`, but uses both via docker.  

Similarly, the [`helm.install` target]({{mkdocs.site_relative_url}}/api#helm.install) works as you'd expect but does not require `helm` (and plus it's a little more idempotent than using `helm` directly).  Meanwhile `k8s.mk k9s/<namespace>` works like `k9s --namespace` does, but doesn't require k9s.

Many of these targets are fairly simple wrappers, but just declaring them accomplishes several things at once.

The typical `k8s.mk` entrypoint is:

1. CLI friendly, for interactive contexts, as above
1. API friendly, for more programmatic use, as part of the prereqs or the body for other project automation
1. Workflow friendly, either as part of `make`'s native DAG processing, or via [flux](/api#api-flux).
1. Potentially a TUI element, via the [embedded TUI](/#embedded-tui) and [tux](/api#tux).
1. Context-agnostic, generally using tools directly if available or falling back to docker when necessary.

Some targets like [`k8s.shell`]({{mkdocs.site_relative_url}}/api/#k8sshell) or [`kubefwd.[start|stop|restart]`]({{mkdocs.site_relative_url}}/api/#kubefwd) are more composite than simple wrappers, and achieve more complex behaviour by orchestrating 1 or more commands across 1 or more containers.  See also the [ansible wrapper]({{mkdocs.site_relative_url}}/api#api-ansible), which exposes a subset of `ansible` without all the overhead of inventories & config.

If you want you can always to stream arbitrary commands or scripts into these containers more directly, via [the Make/Compose bridge]({{jinja.vars.composemk_docs_url}}/bridge), or write your own targets that run inside those containers.  But the point of `k8s.mk` is to ignore more of the low-level details more of the time, and start to compose things.  For example, here's a one-liner that creates a namespace, adds a label to it, launches a pod there, and shells into it:

```bash 
$ pod=`uuidgen` \
&& namespace=testing \
&& ./k8s.mk \
    k8s.kubens.create/${namespace} \ \
    k8s.namespace.label/$${namespace}/mylabel/value
    k8s.test_harness/${namespace}/${pod} \
    k8s.namespace.wait/${namespace} \
    k8s.shell/${namespace}/${pod}
```
### But Why?
<hr style="width:100%;border-bottom:3px solid black;">

There's many reasons why you might want these capabilities if you're working with cluster-lifecycle automation.  People tend to have strong opions about this topic, and it's kind of a long story.  The short version is this: 

* Tool versioning, idempotent operations, & deterministic cluster bootstrapping are all hard problems, but not really the problems we *want* to be working on.
* IDE-plugins and desktop-distros that offer to manage Kubernetes are hard for developers to standardize on, and tend to resist automation.  
* Project-local clusters are much-neglected, but also increasingly important aspects of project testing and overall developer-experience.  
* Ansible/Terraform are great, but they have to be versioned themselves, and aren't necessarily a great fit for this type of problem.  
#}

{#Documentation per-target is included in the next section, but these tools aren't that interesting in isolation.  See the [Cluster Automation Demo](/demos#demo-cluster-automation) for an example of how you can put all this stuff together.#}
