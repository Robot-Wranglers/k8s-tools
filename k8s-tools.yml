#!/usr/bin/env -S docker compose -f
# k8s-tools.yml: 
#
#   Collect, pin & customize versions for your whole k8s toolchain in one place
#   This docker-compose file describes various tool containers, sets up reasonable
#   defaults for volumes, includes fixes for root-container permissions, allows 
#   for version-overrides without editing[2], etc.
#
# DOCS: https://robot-wranglers.github.io/k8s-tools#features
# LATEST: https://robot-wranglers.github.io/k8s-tools/tree/master/k8s-tools.yml
#
# MANIFEST:
#   Local parts of the tool bundle:
#     helmify, kompose, kubefwd, lazydocker,
#     kind, argocli, kn, k3d, k9s, fission, rancher,
#     plus the TUI / DIND base images.
#   Upstream part of the tool bundle (See alpine/k8s docs):
#     kubectl, kustomize, krew, vals, kubeconform, kubeseal, 
#     helm, helm-diff, helm-unittest, helm-push, eksctl, 
#     aws-iam-authenticator, awscli v1
#
# REFS:
#   [1] https://docs.docker.com/compose/environment-variables/set-environment-variables/#cli
#   [2] https://robot-wranglers.github.io/k8s-tools/tree/master/docs/env-vars.md#k8s-toolsyml
#
services:
  k8s: &base
    hostname: k8s-base
    environment: &base_environment
      KUBECONFIG_EXTERNAL: ${KUBECONFIG:-kubeconf.local}
      KUBECONFIG: /home/${DOCKER_UGNAME:-root}/.kube/config
      DOCKER_UID: ${DOCKER_UID:-1000}
      DOCKER_GID: ${DOCKER_GID:-1000}
      DOCKER_UGNAME: ${DOCKER_UGNAME:-root}
      DOCKER_HOST_WORKSPACE: ${DOCKER_HOST_WORKSPACE:-${PWD}}
      MINIKUBE_HOME: /workspace/.minikube
      MINIKUBE_IN_STYLE: 0
      PS1: '\u@\[\033[1;97;4m\]\h\[\033[0m\]:\w\$ '
      TERM: ${TERM:-xterm-256color}
      # Optionally proxy profile.  Usage will require also uncommenting ~/.aws volume
      # AWS_PROFILE: ${AWS_PROFILE:-}
    build:
      tags: ['k8s:base', 'k8s:k8s']
      context: .
      dockerfile_inline: |
        FROM ${ALPINE_K8S_VERSION:-alpine/k8s:1.30.0} AS builder
        RUN cp /krew-* /usr/bin/krew
        FROM ghcr.io/charmbracelet/gum AS gum
        FROM ${ALPINE_K8S_VERSION:-alpine/k8s:1.30.0} 
        COPY --from=gum /usr/local/bin/gum /usr/bin
        COPY --from=builder /usr/bin/krew /usr/bin/
        RUN apk --no-cache add -q procps make ncurses shadow coreutils pv uuidgen
        RUN echo ${DOCKER_GID:-1000} && getent group ${DOCKER_GID:-1000} \
          || groupadd --gid ${DOCKER_GID:-1000} docker
        RUN getent passwd ${DOCKER_UGNAME:-root} || \
          useradd --uid ${DOCKER_UID:-1000} --create-home \
          -g ${DOCKER_GID:-1000} ${DOCKER_UGNAME:-root}
        RUN mkdir -p /home/${DOCKER_UGNAME:-root}/.kube /home/${DOCKER_UGNAME:-root}/.krew /home/${DOCKER_UGNAME:-root}/.config
        RUN KREW_ROOT=/home/${DOCKER_UGNAME:-root}/.krew krew install ctx ns graph sick-pods ktop 2>/dev/null
        RUN KREW_ROOT=/home/${DOCKER_UGNAME:-root}/.krew krew install ${KREW_PLUGINS:-ctx} 2>/dev/null
        RUN cp /home/${DOCKER_UGNAME:-root}/.krew/bin/kubectl-ns /usr/bin/kubens
        RUN cp /home/${DOCKER_UGNAME:-root}/.krew/bin/kubectl-ctx /usr/bin/kubectx
        RUN cp /home/${DOCKER_UGNAME:-root}/.krew/bin/* /usr/bin
        RUN chown -R ${DOCKER_UID:-1000}:${DOCKER_GID:-1000} /home/${DOCKER_UGNAME:-root}/
        RUN curl -sL https://raw.githubusercontent.com/holman/spark/master/spark -o /usr/bin/spark && chmod ugo+x /usr/bin/spark
        RUN apk -q --no-cache add docker docker-compose kubeadm
        RUN curl -fsLo /usr/local/bin/kubetail https://github.com/kubetail-org/kubetail/releases/download/cli%2F${KUBETAIL_VERSION:-v0.7.0}/kubetail-linux-amd64
        RUN chmod o+x /usr/local/bin/kubetail
        USER ${DOCKER_UGNAME:-root}
        ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/${DOCKER_UGNAME:-root}/.krew/bin
    # NB: Left for reference, and possibly required by older versions of docker or certain configurations?  
    # user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    network_mode: host
    working_dir: /workspace
    volumes:
      # Share the docker sock.  Almost everything will need this
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock
      # Share Docker's storage
      - /var/lib/docker:/var/lib/docker  
      # Share /etc/hosts, so tool containers have access to any custom or kubefwd'd DNS
      - /etc/hosts:/etc/hosts:ro
      
      # Share the working directory with containers.  
      # Overrides are allowed for the workspace, which is occasionally useful with DIND
      - ${workspace:-${PWD}}:/workspace
      
      # OSX tmpfiles
      - /var/folders/:/var/folders/
      
      # Optional volumes. 
      # Dirs for `.cache` and `.config` as below are used by helm, maybe others?
      # - ${HOME}/.cache:/home/${DOCKER_UGNAME:-root}/.cache
      # - ${HOME}/.config/helm:/home/${DOCKER_UGNAME:-root}/.config/helm
      # - ${HOME}/.local:/home/${DOCKER_UGNAME:-root}/.local:ro
      
      # Recommended approach for kubeconfig.  
      # Use something like this if you only want to share one file.
      - "${KUBECONFIG:-~/.kube/config}:/home/${DOCKER_UGNAME:-root}/.kube/config"
      
      # Optional volume for whole .kube folder.
      # This is not recommended because it may involve different krew plugins, 
      # and may conflict with simpler usage of local-only KUBECONFIGs, etc
      # - ${HOME}/.kube:/home/${DOCKER_UGNAME:-root}/.kube
      
      # Minikube cache, not shared between projects.
      # - ./.minikube:/workspace/.minikube
      
      # NB: Add this if you're working with EKS and need AWS creds, similarly for Azure
      # - ${HOME}/.aws:/home/${DOCKER_UGNAME:-root}/.aws
    tty: true 

  kubetail_server:
    <<: *base
    hostname: k8s:kubetail
    depends_on: ['k8s']
    command: >- 
      sh -x -c "kubetail serve --skip-open \
      --kubeconfig /home/${DOCKER_UGNAME:-root}/.kube/config \
      --port ${KUBETAIL_PORT:-9999}"
    build: 
      context: . 
      tags: ['k8s:kubetail']
      dockerfile_inline: |
        FROM k8s:base

  minikube:
    <<: *base
    hostname: k8s:minikube 
    entrypoint: minikube 
    depends_on: ['k8s']
    build: 
      context: . 
      tags: ['k8s:minikube']
      dockerfile_inline: |
        FROM k8s:base AS base
        USER root
        RUN apk -q --no-cache add libc6-compat
        RUN curl -fsLo /usr/local/bin/minikube https://github.com/kubernetes/minikube/releases/download/${MINIKUBE_VERSION:-v1.36.0}/minikube-linux-amd64
        RUN chmod o+x /usr/local/bin/minikube
        USER ${DOCKER_UGNAME:-root}
    # volumes:
    #   - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock
    #   # Share Docker's storage
    #   - /var/lib/docker:/var/lib/docker  
    #   # Share /etc/hosts, so tool containers have access to any custom or kubefwd'd DNS
    #   - /etc/hosts:/etc/hosts:ro
    #   # Share the working directory with containers.  
    #   # Overrides are allowed for the workspace, which is occasionally useful with DIND
    #   - ${workspace:-${PWD}}:/workspace
    #   - "${KUBECONFIG:-~/.kube/config}:/home/${DOCKER_UGNAME:-root}/.kube/config"
      
  skupper:
    <<: *base
    hostname: k8s:skupper 
    entrypoint: skupper 
    depends_on: ['k8s']
    build: 
      context: . 
      tags: ['k8s:skupper']
      dockerfile_inline: |
        FROM k8s:base AS base
        USER root
        RUN apk -q --no-cache add libc6-compat
        RUN curl -fsL https://skupper.io/v2/install.sh \
          | sh /dev/stdin --version ${SKUPPER_VERSION:-2.0.0}
        RUN mv ~/.local/bin/skupper /usr/local/bin
        USER ${DOCKER_UGNAME:-root}
  
  # registry-mirror:
  #   image: registry:2
  #   environment:
  #     REGISTRY_PROXY_REMOTEURL: http://host.docker.internal:5000
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"

  # https://helm.sh/docs/
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides, and
  # avoid confusion, but the base can always be used instead.
  helm:
    <<: *base
    entrypoint: helm
    hostname: k8s:helm
  
  # https://kubernetes.io/docs/reference/kubectl/
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides, and
  # avoid confusion, but the base can always be used instead.
  kubectl:
    <<: *base
    entrypoint: kubectl
    hostname: k8s:kubectl

  # https://github.com/aws/aws-cli
  awscli: # v1
    <<: *base
    entrypoint: awscli
    hostname: k8s:awscli
    build: 
      context: . 
      tags: ['k8s:awscli']
      dockerfile_inline: |
        # FROM amazon/aws-cli:2.27.34 as aws
        FROM k8s:base AS base
        # USER root
        # COPY --from=aws /usr/local/aws-cli/ /usr/local/aws-cli/
        # COPY --from=aws /usr/local/bin/aws /usr/local/bin/aws
        # # Install additional packages
        # RUN apk add --no-cache curl jq bash python3
        # RUN apk add -q --no-cache libc6-compat musl-dev
        # ENV PATH="/usr/local/aws-cli/v2/current/bin:$PATH"
        # USER ${DOCKER_UGNAME:-root}
    # docker pull amazon/aws-cli:2.27.34
  # https://docs.aws.amazon.com/cdk
  # /usr/local/bin/aws
  
  # FIXME: needs a python stack for many use-cases
  cdk:
    <<: *base
    hostname: k8s:cdk 
    entrypoint: cdk 
    build: 
      tags: ['k8s:cdk']
      context: . 
      dockerfile_inline: |
        FROM node:lts-iron
        RUN npm i -g aws-cdk@${CDK_CLI_VERSION:-2.149.0}
    
  # https://github.com/bitnami-labs/sealed-secrets
  # kubeseal:
  #   <<: *base
  #   entrypoint: kubeseal
  #   hostname: k8s:kubeseal

  # https://github.com/kubernetes-sigs/krew
  # krew:
  #   <<: *base
  #   entrypoint: krew
  #   hostname: k8s:krew

  # https://github.com/helmfile/vals
  # vals:
  #   <<: *base
  #   entrypoint: vals
  #   hostname: k8s:vals

  # https://github.com/yannh/kubeconform
  # kubeconform:
  #   <<: *base
  #   entrypoint: kubeconform
  #   hostname: k8s:kubeconform

  # https://knative.dev/docs/client/install-kn/
  kn: &knative
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:kn
    build:
      tags: ['k8s:kn']
      context: .
      dockerfile_inline: |
        FROM k8s:base AS base
        FROM ghcr.io/knative/func/func AS builder
        FROM gcr.io/knative-releases/knative.dev/client/cmd/kn:${KN_CLI_VERSION:-v1.14.0}
        COPY --from=builder /ko-app/func /ko-app/func
        COPY --from=base /usr/bin/kubectl /usr/bin/
        RUN apk --no-cache add bash procps make
        RUN cp /ko-app/func /usr/bin/kn-func

  # https://hub.docker.com/r/istio/istioctl/
  istioctl:
    <<: *base
    entrypoint: istioctl
    depends_on: ['k8s']
    hostname: k8s:istioctl
    build:
      tags: ['k8s:istioctl']
      context: .
      dockerfile_inline: |
        FROM istio/istioctl:${ISTIO_CONTAINER_VERSION:-1.24.5} AS builder
        FROM k8s:base AS base
        COPY --from=builder /usr/local/bin/istioctl /usr/local/bin/istioctl

  # https://github.com/arttor/helmify
  helmify:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:helmify
    build:
      tags: ['k8s:helmify']
      context: .
      dockerfile_inline: |
        FROM ${DEBIAN_CONTAINER_VERSION:-debian:bookworm}
        RUN apt-get update -qq && apt-get install -qq -y curl
        RUN cd /tmp && curl -s -Lo helmify.tgz \
            https://github.com/arttor/helmify/releases/download/${HELMIFY_CLI_VERSION:-v0.4.12}/helmify_Linux_i386.tar.gz
        RUN cd /tmp && tar -zxvf helmify.tgz && chmod +x helmify && mv helmify /usr/local/bin/
    entrypoint: helmify
    tty: false
    stdin_open: true

  # https://fission.io/docs/installation/
  fission:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:fission
    build:
      tags: ['k8s:fission']
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root 
        RUN curl -s -Lo fission \
            https://github.com/fission/fission/releases/download/${FISSION_CLI_VERSION:-v1.21.0}/fission-${FISSION_CLI_VERSION:-v1.21.0}-linux-amd64
        RUN chmod +x fission && mv fission /usr/local/bin/
        USER ${DOCKER_UGNAME:-root}
    entrypoint: fission

  # https://github.com/kubernetes/kompose/blob/main/docs/installation.md#github-release
  kompose:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:kompose
    build:
      tags: ['k8s:kompose']
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root 
        RUN curl -Lo /usr/bin/kompose https://github.com/kubernetes/kompose/releases/download/${KOMPOSE_CLI_VERSION:-v1.36.0}/kompose-linux-amd64
        RUN chmod +x /usr/bin/kompose
        USER ${DOCKER_UGNAME:-root}
    entrypoint: kompose
  
  # https://argo-workflows.readthedocs.io/en/latest/walk-through/argo-cli/
  # FIXME: pin version
  argo:
    <<: *base 
    depends_on: ['k8s']
    hostname: k8s:argo
    build:
      tags: ['k8s:argo']
      context: . 
      dockerfile_inline: |
        FROM quay.io/argoproj/argocli:${ARGO_CLI_VERSION:-v3.4.17} AS argo
        FROM k8s:base 
        COPY --from=argo /bin/argo /bin/argo
    entrypoint: argo 
  
  # https://docs.tigera.io/calico/latest/operations/calicoctl/install
  calicoctl:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:calicoctl
    build:
      tags: ['k8s:calicoctl']
      context: .
      dockerfile_inline: |
        FROM calico/ctl:${CALICO_VERSION:-v3.29.3}
        FROM k8s:base
        USER root
        RUN curl -Lo /usr/local/bin/calicoctl https://github.com/projectcalico/calico/releases/download/${CALICO_VERSION:-v3.29.3}/calicoctl-linux-amd64
        RUN chmod +x /usr/local/bin/calicoctl
        USER ${DOCKER_UGNAME:-root}
    entrypoint: /usr/local/bin/calicoctl
  
  kubefwd:
    <<: *base 
    depends_on: ['k8s']
    hostname: k8s:kubefwd
    user: root 
    build:
      tags: ['k8s:kubefwd']
      context: . 
      dockerfile_inline: |
        FROM txn2/kubefwd:${KUBEFWD_VERSION:-1.22.5} AS builder 
        FROM k8s:base
        COPY --from=builder /kubefwd /usr/bin/kubefwd
    entrypoint: kubefwd
    environment:
      <<: *base_environment
      KUBECONFIG_EXTERNAL: ${KUBECONFIG:-kubeconf.local}
      KUBECONFIG: /root/.kube/config
    volumes: 
      # Same as the base volumes, plus /etc/hosts for kubefwd to sync DNS
      - /etc/hosts:/etc/hosts:rw
      - ${workspace:-${PWD}}:/workspace
      - ${HOME}/.cache:/root/.cache
      - "${KUBECONFIG:-~/.kube/config}:/root/.kube/config"
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock
      # Share /etc/hosts, so tool containers have access to any custom or kubefwd'd DNS
      # - ${workspace:-${PWD}}:/workspace
      - /var/folders/:/var/folders/

  # https://k3d.io/
  k3d:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:k3d
    build:
      tags: ['k8s:k3d']
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root
        RUN curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh \
          | TAG=${K3D_VERSION:-v5.8.3} bash
        USER ${DOCKER_UGNAME:-root}
    entrypoint: k3d
  
  # https://github.com/jesseduffield/lazydocker
  lazydocker:
    <<: *base
    depends_on: ['k8s']
    hostname: k8s:lazydocker
    build:
      tags: ['k8s:lazydocker']
      context: .
      dockerfile_inline: |
        FROM k8s:base
        USER root
        RUN wget https://github.com/jesseduffield/lazydocker/releases/download/v${LAZY_DOCKER_VERSION:-0.23.1}/lazydocker_${LAZY_DOCKER_VERSION:-0.23.1}_Linux_x86_64.tar.gz 
        RUN tar -zxvf lazydocker*
        RUN mv lazydocker /usr/bin && rm lazydocker*
        USER ${DOCKER_UGNAME:-root}
    entrypoint: lazydocker

  # https://github.com/kubernetes-sigs/kind
  kind: 
    <<: *base
    hostname: "k8s:kind"
    build:
      tags: ['k8s:kind']
      context: .
      dockerfile_inline: |
        FROM k8s:base AS base
        USER root
        RUN curl -fsSL -o/usr/local/bin/kind https://kind.sigs.k8s.io/dl/${KIND_CLI_VERSION:-v0.27.0}/kind-linux-amd64
        RUN chmod +x /usr/local/bin/kind
        USER ${DOCKER_UGNAME:-root}
    entrypoint: /usr/local/bin/kind

  # https://k9scli.io/
  k9s:
    <<: *base 
    hostname: k8s:k9s
    build:
      tags: ['k8s:k9s']
      dockerfile_inline: |
        # Needs bugfix for missing 'infocmp' in latest release?
        FROM derailed/k9s:${K9S_VERSION:-v0.32.4}
        RUN apk add -q ncurses
        RUN which infocmp
    tty: true
    network_mode: host
    entrypoint: k9s
  
  # https://github.com/rancher/cli
  rancher:
    <<: *base 
    depends_on: ['k8s']
    hostname: k8s:rancher
    build:
      tags: ['k8s:rancher']
      context: . 
      dockerfile_inline: |
        FROM rancher/cli2:${RANCHER_CLI_VERSION:-v2.8.4} AS rancher
        FROM k8s:base 
        COPY --from=rancher /usr/bin/rancher /usr/bin/
    entrypoint: /usr/bin/rancher 

  # promtool: https://prometheus.io/docs/prometheus/latest/command-line/promtool/
  promtool:
    <<: *base 
    depends_on: ['k8s']
    hostname: k8s:promtool
    build:
      tags: ['k8s:promtool']
      context: . 
      dockerfile_inline: |
        FROM prom/prometheus:${PROMETHEUS_CLI_VERSION:-v2.52.0} AS prom
        FROM k8s:base 
        COPY --from=prom /bin/promtool /usr/bin/
        USER ${DOCKER_UGNAME:-root}
    entrypoint: promtool
  
  # https://github.com/tsub/docker-graph-easy
  graph-easy:
    image: tsub/graph-easy
    working_dir: ${workspace:-/workspace} 
    volumes: 
      - ${PWD}:${workspace:-/workspace}

  # https://robot-wranglers.github.io/k8s-tools/#embedded-tui
  dind: &dind
    <<: *base 
    depends_on: ['k8s']
    hostname: k8s:dind
    build:
      tags: ['k8s:dind']
      context: . 
      dockerfile_inline: |
        FROM k8s:base AS base
        FROM compose.mk:dind_base 
        COPY --from=base \
          /usr/bin/kube* /usr/bin/helm* \
          /usr/bin/krew /usr/bin/gum \
          /usr/bin 
        USER ${DOCKER_UGNAME:-root}
    entrypoint: bash
  
  # https://hub.docker.com/r/google/cloud-sdk
  gcloud:
    <<: *base
    depends_on: ['k8s']
    hostname: "k8s:gcloud"
    build:
      tags: ['k8s:gcloud']
      context: .
      dockerfile_inline: |
        FROM google/cloud-sdk:${GCLOUD_SDK_VERSION:-526.0.1}
  
  # https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html
  ansible:
    <<: *base
    hostname: k8s:ansible
    depends_on: ['dind']
    tty: true 
    build: 
      # Ansible has no official images and doesn't work well on alpine :(
      # So kubectl/helm versions here are chosen to match current alpine k8s:base,
      # but they don't track automatically if things are updated!
      context: .
      tags: ['k8s:ansible']
      dockerfile_inline: |
        FROM k8s:dind
        USER root
        RUN apt-get update -qq && apt-get install -qq -y python3 python3-pip
        RUN pip3 install --break-system-packages ansible==${ANSIBLE_VERSION:-10.1.0}
        RUN ansible-galaxy collection install kubernetes.core
        RUN pip install kubernetes --break-system-packages
        RUN pip freeze|grep kuber
        USER ${DOCKER_UGNAME:-root}
  
  # https://submariner.io/operations/deployment/subctl/
  subctl:
    <<: *base
    depends_on: ['k8s']
    hostname: "k8s:subctl"
    build:
      tags: ['k8s:subctl']
      context: .
      dockerfile_inline: |
        FROM k8s:base AS base
        USER root
        RUN curl -Ls https://get.submariner.io | VERSION=${SUBCTL_CLI_VERSION:-v0.20.0} bash
        RUN mv ~/.local/bin/subctl /usr/local/bin/subctl
        USER ${DOCKER_UGNAME:-root}
    entrypoint: /usr/local/bin/subctl
  
  # https://tilt.dev/
  tilt:
    <<: *base
    depends_on: ['k8s']
    hostname: "k8s:tilt"
    build:
      tags: ['k8s:tilt']
      context: .
      dockerfile_inline: |
        # FROM docker/tilt:v${TILT_VERSION:-0.34.5} AS tilt
        #COPY --from=tilt /usr/local/bin/tilt /usr/local/bin/tilt
        FROM k8s:base 
        USER root 
        RUN apk add -q --no-cache libc6-compat shadow
        RUN curl -fsSL \
          https://github.com/tilt-dev/tilt/releases/download/v${TILT_VERSION:-0.34.5}/tilt.${TILT_VERSION:-0.34.5}.linux.x86_64.tar.gz \
          | tar -xzv tilt && mv tilt /usr/local/bin/tilt
        USER ${DOCKER_UGNAME:-root}
    command: >-
      sh -x -c "export TILT_DISABLE_ANALYTICS=true; tilt down -f ${TILT_FILE:-Tiltfile}; tilt up -f ${TILT_FILE:-Tiltfile} --port ${TILT_PORT:-10350} --host ${TILT_HOST:-0.0.0.0} --stream"
  
  # https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs
  terraform:
    <<: *base
    depends_on: ['k8s']
    hostname: "k8s:terraform"
    build:
      tags: ['k8s:terraform']
      context: .
      dockerfile_inline: |
        FROM hashicorp/terraform:${TERAFORM_VERSION:-1.12.2} as tf_base
        FROM k8s:base 
        USER root 
        COPY --from=tf_base /bin/terraform /usr/local/bin/terraform
        USER ${DOCKER_UGNAME:-root}
  
  # https://robot-wranglers.github.io/compose.mk/#embedded-tui
  tui: 
    <<: *base
    hostname: k8s:tui
    environment:
      <<: *base_environment
      CMK_DIND: "1"
      KUBECONFIG: "${KUBECONFIG:-~/.kube/config}"
      TMUX: "${TUI_TMUX_SOCKET:-/workspace/tmux.sock}"
    image: k8s:tui
    build:
      tags: ['k8s:tui']
      context: . 
      dockerfile_inline: |
        FROM k8s:dind AS dind_base
        FROM compose.mk:tux
        COPY --from=dind_base \
          /usr/bin/kube* /usr/bin/helm* \
          /usr/bin/krew /usr/bin/gum \
          /usr/bin 
        USER root 
        RUN apt-get update -qq && apt-get install -qq -y graphviz imagemagick
        USER ${DOCKER_UGNAME:-root}

  # NB: All of the following are already available in the k8s base,
  # i.e. from alpine-k8s. The services below are just stubs to accomodate 
  # overrides.  
  #░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

  # https://github.com/kubernetes-sigs/kustomize
  # kustomize:
  #   <<: *base
  #   entrypoint: kustomize
  #   hostname: k8s:kustomize

  # https://github.com/databus23/helm-diff
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides.
  # helm-diff:
  #   <<: *base
  #   entrypoint: helm-diff
  #   hostname: k8s:helm-diff

  # https://github.com/helm-unittest/helm-unittest
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides.
  # helm-unittest:
  #   <<: *base
  #   entrypoint: helm-unittest
  #   hostname: k8s:helm-unittest

  # https://github.com/chartmuseum/helm-push
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides.
  # helm-push:
  #   <<: *base
  #   entrypoint: helm-push
  #   hostname: k8s:helm-push

  # https://github.com/kubernetes-sigs/aws-iam-authenticator
  # Already available in the k8s base (from alpine-k8s).
  # This service is just a stub to accomodate overrides.
  # aws-iam-authenticator:
  #   <<: *base
  #   entrypoint: aws-iam-authenticator
  #   hostname: k8s:aws-iam-authenticator
  #   tty: true

  # https://github.com/weaveworks/eksctl
  # eksctl:
  #   <<: *base
  #   entrypoint: eksctl
  #   hostname: k8s:eksctl
  
  ## Experimental 
  #░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░
  # metaflow:
  #   <<: *base
  #   depends_on: ['k8s']
  #   hostname: "k8s:metaflow"
  #   build:
  #     tags: ['k8s:metaflow']
  #     context: .
  #     dockerfile_inline: |
  #       FROM k8s:base 
  #       USER root 
  #       RUN apk add -q --update --no-cache \
  #         python3 py3-pip curl sudo
  #       RUN pip install \
  #         metaflow==2.15.15 kubernetes==32.0.1 pylint==3.3.7 \
  #         --break-system-packages
  #       RUN apk add -q --update --no-cache \
  #         bash 
  #       USER ${DOCKER_UGNAME:-root}
  
  # buildah:
  #   <<: *base
  #   depends_on: ['k8s']
  #   hostname: "k8s:buildah"
  #   build:
  #     tags: ['k8s:buildah']
  #     context: .
  #     dockerfile_inline: |
  #       # FROM quay.io/buildah/stable:v1.40.0 as base 
  #       FROM k8s:base
  #       USER root
  #       RUN apk add --no-cache \
  #           bash git go build-base \
  #           gcompat btrfs-progs-dev gpgme-dev \
  #           fuse-overlayfs \
  #           libseccomp-dev
  #       # Clone and build Buildah
  #       RUN mkdir -p /opt 
  #       RUN git clone --depth 1 \
  #         --branch ${BUILDAH_VERSION:-v1.36.0} \
  #           https://github.com/containers/buildah.git /opt/buildah
  #       RUN cd /opt/buildah && make && make install 
  #       RUN apk add --no-cache netavark podman 
  #   privileged: true
  #   environment:
  #     - BUILDAH_RUNTIME=runc
  #     - BUILDAH_ISOLATION=oci
  #     - STORAGE_DRIVER=overlay2
  #     - STORAGE_OPTS=overlay2.override_kernel_check=1
  #   entrypoint: /usr/local/bin/buildah